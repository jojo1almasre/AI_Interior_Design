{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lb4CGzhnJyXI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "169926f9-e74f-40a3-95d6-883bb1767f01"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/511.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m511.6/511.6 kB\u001b[0m \u001b[31m18.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.9/8.9 MB\u001b[0m \u001b[31m50.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.7/47.7 MB\u001b[0m \u001b[31m18.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip -q install \"pillow<12\"\n",
        "!pip -q install -U transformers datasets evaluate accelerate opencv-python scipy scikit-learn\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# تثبيت SAM (مرة واحدة)\n",
        "!pip -q install git+https://github.com/facebookresearch/segment-anything.git\n",
        "!pip -q install -U pycocotools opencv-python matplotlib"
      ],
      "metadata": {
        "id": "IuYBKvRQaUll",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3eded261-28c7-4e76-a289-4246c72e2295"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for segment_anything (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.8/52.8 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.7/8.7 MB\u001b[0m \u001b[31m46.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "import urllib\n",
        "import os, glob, numpy as np\n",
        "import urllib.request\n",
        "from PIL import Image\n",
        "import random\n",
        "import cv2\n",
        "from scipy.io import loadmat\n",
        "import glob, random\n",
        "from collections import Counter\n",
        "from sklearn.model_selection import train_test_split\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.utils.data import Dataset\n",
        "from transformers import SegformerImageProcessor, SegformerForSemanticSegmentation\n",
        "from transformers import TrainingArguments, Trainer\n",
        "import evaluate\n",
        "import torch\n",
        "from segment_anything import sam_model_registry, SamPredictor\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from transformers import SegformerImageProcessor, SegformerForSemanticSegmentation"
      ],
      "metadata": {
        "id": "T7t2BhelQ5zU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount(\"/content/drive\")"
      ],
      "metadata": {
        "id": "QQIodHbORYqD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ce01b688-8070-4c62-d359-cc43d1dc6a6f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "DRIVE_ROOT = \"/content/drive/MyDrive/NYU_Depth_Dataset\"\n",
        "DRIVE_ROOT1 = \"/content/drive/MyDrive/NYU_Depth_Dataset/output\"\n",
        "IMG_DIR   = os.path.join(DRIVE_ROOT1, \"images\")   #  images\n",
        "LABEL_DIR = os.path.join(DRIVE_ROOT1, \"labels\")   #  labels\n",
        "MAT_PATH  = os.path.join(DRIVE_ROOT, \"nyu_depth_v2_labeled.mat\")\n",
        "\n",
        "assert os.path.isdir(IMG_DIR), IMG_DIR\n",
        "assert os.path.isdir(LABEL_DIR), LABEL_DIR\n",
        "assert os.path.isfile(MAT_PATH), f\"ملف .mat غير موجود: {MAT_PATH}\"\n",
        "\n",
        "print(\"IMG_DIR:\", IMG_DIR)\n",
        "print(\"LABEL_DIR:\", LABEL_DIR)\n",
        "print(\"MAT_PATH:\", MAT_PATH)"
      ],
      "metadata": {
        "id": "iQHcIjoZRc1C",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5fea3b97-a51b-495b-e77c-d6db5b8b67b5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "IMG_DIR: /content/drive/MyDrive/NYU_Depth_Dataset/output/images\n",
            "LABEL_DIR: /content/drive/MyDrive/NYU_Depth_Dataset/output/labels\n",
            "MAT_PATH: /content/drive/MyDrive/NYU_Depth_Dataset/nyu_depth_v2_labeled.mat\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#فحص سريع للـ labels\n",
        "def load_id_mask(path):\n",
        "    m = np.array(Image.open(path))\n",
        "    if m.ndim == 3:\n",
        "        m = m[...,0]\n",
        "    return m\n",
        "\n",
        "sample = random.choice(glob.glob(os.path.join(LABEL_DIR, \"*.png\")))\n",
        "m = load_id_mask(sample)\n",
        "print(\"Sample:\", os.path.basename(sample))\n",
        "print(\"Shape:\", m.shape)\n",
        "print(\"Unique IDs (first 50):\", np.unique(m)[:50])\n",
        "print(\"Max ID:\", int(m.max()))"
      ],
      "metadata": {
        "id": "L1zB7hWORoER",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "62132f18-c4df-4e6b-de54-f5a96419119a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample: label_1001.png\n",
            "Shape: (480, 640)\n",
            "Unique IDs (first 50): [  0   5  11  21  28  64  84  85  88  89  98  99 119 137 141 157 177 255]\n",
            "Max ID: 255\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import h5py\n",
        "\n",
        "f = h5py.File(MAT_PATH, \"r\")\n",
        "\n",
        "print(\"Top/H5 keys:\", list(f.keys()), \"... total:\", len(f.keys()))\n",
        "\n",
        "# ---- 1) قراءة أسماء الفئات من names (References) ----\n",
        "assert \"names\" in f, \"ما لقيت المفتاح 'names' داخل الملف.\"\n",
        "\n",
        "names_refs = np.array(f[\"names\"]).squeeze()  # غالباً array من object references\n",
        "\n",
        "def ref_to_string(h5file, ref):\n",
        "    \"\"\"\n",
        "    يفك HDF5 object reference ويحوّله لنص.\n",
        "    كثير ملفات MATLAB v7.3 تخزن النص كسلسلة char codes (uint16) أو bytes.\n",
        "    \"\"\"\n",
        "    obj = h5file[ref]              # Dataset أو Group مشار إليه\n",
        "    arr = np.array(obj)\n",
        "\n",
        "    # 1) إذا كان bytes مباشرة\n",
        "    if arr.dtype.kind in (\"S\",):   # byte strings\n",
        "        s = b\"\".join(arr.flatten()).decode(\"utf-8\", errors=\"ignore\")\n",
        "        return s.strip(\"\\x00\").strip()\n",
        "\n",
        "    # 2) MATLAB chars غالباً uint16 (Unicode code units)\n",
        "    if arr.dtype == np.uint16 or arr.dtype == np.uint8 or arr.dtype == np.int16:\n",
        "        # MATLAB يخزن char arrays بشكل عمودي أحياناً -> جرّب order='F'\n",
        "        flat = arr.flatten(order=\"F\")\n",
        "        # حوّل لأحرف\n",
        "        s = \"\".join(chr(int(c)) for c in flat if int(c) != 0)\n",
        "        return s.strip()\n",
        "\n",
        "    # 3) fallback\n",
        "    return str(arr.squeeze())\n",
        "\n",
        "# بعض الملفات يكون names_refs على شكل 2D (1,N) أو (N,1)\n",
        "names_refs_flat = names_refs.flatten()\n",
        "\n",
        "class_names = [ref_to_string(f, r) for r in names_refs_flat]\n",
        "print(\"Example class names:\", class_names[:30])\n",
        "\n",
        "# ---- 2) إيجاد IDs عبر كلمات مفتاحية ----\n",
        "def find_ids_by_keywords(names, keywords):\n",
        "    ids = []\n",
        "    for i, n in enumerate(names):\n",
        "        ln = (n or \"\").lower()\n",
        "        if any(kw in ln for kw in keywords):\n",
        "            ids.append(i)\n",
        "    return ids\n",
        "\n",
        "wall_ids_raw    = find_ids_by_keywords(class_names, [\"wall\"])\n",
        "floor_ids_raw   = find_ids_by_keywords(class_names, [\"floor\"])\n",
        "ceiling_ids_raw = find_ids_by_keywords(class_names, [\"ceiling\", \"ceil\"])\n",
        "\n",
        "print(\"Raw matches counts:\", {\n",
        "    \"wall\": len(wall_ids_raw),\n",
        "    \"floor\": len(floor_ids_raw),\n",
        "    \"ceiling\": len(ceiling_ids_raw),\n",
        "})\n",
        "print(\"Some wall matches:\", [class_names[i] for i in wall_ids_raw[:10]])\n",
        "print(\"Some floor matches:\", [class_names[i] for i in floor_ids_raw[:10]])\n",
        "print(\"Some ceiling matches:\", [class_names[i] for i in ceiling_ids_raw[:10]])\n",
        "\n",
        "assert len(wall_ids_raw) > 0 and len(floor_ids_raw) > 0 and len(ceiling_ids_raw) > 0, \\\n",
        "    \"لسّا ما لقيت wall/floor/ceiling. ابعتيلي أول 80 اسم من class_names لنضبط كلمات البحث.\"\n",
        "\n",
        "# ---- 3) كشف 0-based / 1-based من PNG labels ----\n",
        "png_paths = glob.glob(os.path.join(LABEL_DIR, \"*.png\"))\n",
        "assert len(png_paths) > 0, f\"ما لقيت PNG داخل {LABEL_DIR}\"\n",
        "\n",
        "png_sample = load_id_mask(random.choice(png_paths))\n",
        "uniq = set(np.unique(png_sample).tolist())\n",
        "\n",
        "def pick_base(raw_ids, uniq_set):\n",
        "    if any(i in uniq_set for i in raw_ids):\n",
        "        return 0\n",
        "    if any((i + 1) in uniq_set for i in raw_ids):\n",
        "        return 1\n",
        "    return 0\n",
        "\n",
        "BASE = max(\n",
        "    pick_base(wall_ids_raw, uniq),\n",
        "    pick_base(floor_ids_raw, uniq),\n",
        "    pick_base(ceiling_ids_raw, uniq),\n",
        ")\n",
        "\n",
        "print(\"Detected label base (0=0-based, 1=1-based):\", BASE)\n",
        "\n",
        "WALL_IDS    = [i + BASE for i in wall_ids_raw]\n",
        "FLOOR_IDS   = [i + BASE for i in floor_ids_raw]\n",
        "CEILING_IDS = [i + BASE for i in ceiling_ids_raw]\n",
        "\n",
        "print(\"Candidate WALL IDs:\", WALL_IDS[:15])\n",
        "print(\"Candidate FLOOR IDs:\", FLOOR_IDS[:15])\n",
        "print(\"Candidate CEILING IDs:\", CEILING_IDS[:15])\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "HBKZcFdtRrin",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f97500fb-a5c5-4fdc-ab80-83f592a4832a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top/H5 keys: ['#refs#', '#subsystem#', 'accelData', 'depths', 'images', 'instances', 'labels', 'names', 'namesToIds', 'rawDepthFilenames', 'rawDepths', 'rawRgbFilenames', 'sceneTypes', 'scenes'] ... total: 14\n",
            "Example class names: ['book', 'bottle', 'cabinet', 'ceiling', 'chair', 'cone', 'counter', 'dishwasher', 'faucet', 'fire extinguisher', 'floor', 'garbage bin', 'microwave', 'paper towel dispenser', 'paper', 'pot', 'refridgerator', 'stove burner', 'table', 'unknown', 'wall', 'bowl', 'magnet', 'sink', 'air vent', 'box', 'door knob', 'door', 'scissor', 'tape dispenser']\n",
            "Raw matches counts: {'wall': 6, 'floor': 3, 'ceiling': 1}\n",
            "Some wall matches: ['wall', 'wall decoration', 'wall stand', 'wall hand sanitizer dispenser', 'wallet', 'wall divider']\n",
            "Some floor matches: ['floor', 'floor mat', 'floor trim']\n",
            "Some ceiling matches: ['ceiling']\n",
            "Detected label base (0=0-based, 1=1-based): 1\n",
            "Candidate WALL IDs: [21, 186, 295, 440, 661, 800]\n",
            "Candidate FLOOR IDs: [11, 143, 868]\n",
            "Candidate CEILING IDs: [4]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def find_ids_regex(names, pattern):\n",
        "    rgx = re.compile(pattern, flags=re.IGNORECASE)\n",
        "    return [i for i, n in enumerate(names) if rgx.search(n)]\n",
        "\n",
        "# floor: نريد \"floor\" لوحدها أو يبدأ فيها\n",
        "floor_ids_raw = find_ids_regex(class_names, r\"^floor$|^floor\\s\")\n",
        "\n",
        "# ceiling: غالبًا \"ceiling\" لوحدها\n",
        "ceiling_ids_raw = find_ids_regex(class_names, r\"^ceiling$\")\n",
        "\n",
        "# wall: نريد \"wall\" لوحدها أو يبدأ فيها \"wall ...\"\n",
        "# (هذا يستبعد wallet لأنه يبدأ بـ wal**let** وليس wall + مسافة)\n",
        "wall_ids_raw = find_ids_regex(class_names, r\"^wall$|^wall\\s\")\n",
        "\n",
        "print(\"Filtered matches:\", {\n",
        "    \"wall\": (len(wall_ids_raw), [class_names[i] for i in wall_ids_raw]),\n",
        "    \"floor\": (len(floor_ids_raw), [class_names[i] for i in floor_ids_raw]),\n",
        "    \"ceiling\": (len(ceiling_ids_raw), [class_names[i] for i in ceiling_ids_raw]),\n",
        "})\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Kw5UQhYD376",
        "outputId": "5055a04b-e20b-43ea-bbb0-18fda5bc6751"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Filtered matches: {'wall': (5, ['wall', 'wall decoration', 'wall stand', 'wall hand sanitizer dispenser', 'wall divider']), 'floor': (3, ['floor', 'floor mat', 'floor trim']), 'ceiling': (1, ['ceiling'])}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "BASE = 1\n",
        "\n",
        "WALL_IDS    = [i + BASE for i in wall_ids_raw]\n",
        "FLOOR_IDS   = [i + BASE for i in floor_ids_raw]\n",
        "CEILING_IDS = [i + BASE for i in ceiling_ids_raw]\n",
        "\n",
        "print(\"FINAL WALL IDS:\", WALL_IDS)\n",
        "print(\"FINAL FLOOR IDS:\", FLOOR_IDS)\n",
        "print(\"FINAL CEILING IDS:\", CEILING_IDS)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vnmN4esJD9y8",
        "outputId": "e5cff240-5f98-48dc-e20d-2afb3c9597b5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FINAL WALL IDS: [21, 186, 295, 440, 800]\n",
            "FINAL FLOOR IDS: [11, 143, 868]\n",
            "FINAL CEILING IDS: [4]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#اختيار ID “الأقوى” لكل فئة حسب تكراره + تحويل كل الـ labels إلى 0/1/2/3\n",
        "label_files = sorted(glob.glob(os.path.join(LABEL_DIR, \"*.png\")))\n",
        "assert len(label_files) > 0\n",
        "\n",
        "cnt = Counter()\n",
        "for p in label_files[:200]:  # كفاية 200 للتقدير (سريع)\n",
        "    src = load_id_mask(p)\n",
        "    vals, counts = np.unique(src, return_counts=True)\n",
        "    for v, c in zip(vals, counts):\n",
        "        cnt[int(v)] += int(c)\n",
        "\n",
        "def best_id(candidates):\n",
        "    candidates = [c for c in candidates if c in cnt]\n",
        "    assert len(candidates) > 0, \"ولا ID من المرشحين ظهر بالليبلات!\"\n",
        "    return max(candidates, key=lambda x: cnt[x])\n",
        "\n",
        "BEST_WALL    = best_id(WALL_IDS)\n",
        "BEST_FLOOR   = best_id(FLOOR_IDS)\n",
        "BEST_CEILING = best_id(CEILING_IDS)\n",
        "\n",
        "print(\"BEST_WALL:\", BEST_WALL, \"BEST_FLOOR:\", BEST_FLOOR, \"BEST_CEILING:\", BEST_CEILING)\n",
        "\n",
        "OUT_MASK_DIR = os.path.join(DRIVE_ROOT1, \"labels_0_1_2_3\")\n",
        "os.makedirs(OUT_MASK_DIR, exist_ok=True)\n",
        "\n",
        "for p in label_files:\n",
        "    src = load_id_mask(p)\n",
        "    out = np.zeros_like(src, dtype=np.uint8)  # background=0\n",
        "    out[src == BEST_WALL]    = 1\n",
        "    out[src == BEST_FLOOR]   = 2\n",
        "    out[src == BEST_CEILING] = 3\n",
        "    Image.fromarray(out, mode=\"L\").save(os.path.join(OUT_MASK_DIR, os.path.basename(p)))\n",
        "\n",
        "chk = load_id_mask(os.path.join(OUT_MASK_DIR, os.path.basename(label_files[0])))\n",
        "print(\"Converted mask unique:\", np.unique(chk))\n",
        "print(\"Saved masks to:\", OUT_MASK_DIR)"
      ],
      "metadata": {
        "id": "zuA7_umoSIuk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8eea0d65-4a09-455a-f708-b3674b48cb24"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BEST_WALL: 21 BEST_FLOOR: 11 BEST_CEILING: 4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-817774387.py:32: DeprecationWarning: 'mode' parameter is deprecated and will be removed in Pillow 13 (2026-10-15)\n",
            "  Image.fromarray(out, mode=\"L\").save(os.path.join(OUT_MASK_DIR, os.path.basename(p)))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Converted mask unique: [0 1 2 3]\n",
            "Saved masks to: /content/drive/MyDrive/NYU_Depth_Dataset/output/labels_0_1_2_3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1) دالة تستخرج رقم/ID من اسم الملف (أول رقم بتلاقيه)\n",
        "def extract_id(filename: str):\n",
        "    m = re.search(r\"\\d+\", filename)\n",
        "    return m.group(0) if m else None\n",
        "\n",
        "img_files = sorted([f for f in os.listdir(IMG_DIR)\n",
        "                    if f.lower().endswith((\".jpg\", \".jpeg\", \".png\"))])\n",
        "\n",
        "msk_files = sorted([f for f in os.listdir(OUT_MASK_DIR)\n",
        "                    if f.lower().endswith(\".png\")])\n",
        "\n",
        "mask_map = {}\n",
        "for m in msk_files:\n",
        "    mid = extract_id(m)\n",
        "    if mid:\n",
        "        mask_map[mid] = os.path.join(OUT_MASK_DIR, m)\n",
        "\n",
        "pairs = []\n",
        "missing = 0\n",
        "for im in img_files:\n",
        "    iid = extract_id(im)\n",
        "    if iid and iid in mask_map:\n",
        "        pairs.append((os.path.join(IMG_DIR, im), mask_map[iid]))\n",
        "    else:\n",
        "        missing += 1\n",
        "\n",
        "print(\"Images:\", len(img_files), \"Masks:\", len(msk_files))\n",
        "print(\"Matched pairs:\", len(pairs), \"Missing images without mask:\", missing)\n",
        "\n",
        "assert len(pairs) > 50, \"المطابقات قليلة جدًا—راجعي أسماء الصور/الماسكات واستخراج الـ id.\"\n",
        "\n",
        "train_pairs, val_pairs = train_test_split(pairs, test_size=0.15, random_state=42)\n",
        "print(\"Train:\", len(train_pairs), \"Val:\", len(val_pairs))"
      ],
      "metadata": {
        "id": "VgrZwHZgSQhc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "70800862-0dd6-4970-e9af-be1d675d7ff5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Images: 1449 Masks: 1449\n",
            "Matched pairs: 1449 Missing images without mask: 0\n",
            "Train: 1231 Val: 218\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# تدريب SegFormer بشكل قوي (Augment + fp16 + cosine scheduler) + حفظ على Drive\n",
        "id2label = {0:\"background\", 1:\"wall\", 2:\"floor\", 3:\"ceiling\"}\n",
        "label2id = {v:k for k,v in id2label.items()}\n",
        "\n",
        "CKPT = \"nvidia/segformer-b2-finetuned-ade-512-512\"\n",
        "\n",
        "processor = SegformerImageProcessor.from_pretrained(CKPT)\n",
        "model = SegformerForSemanticSegmentation.from_pretrained(\n",
        "    CKPT,\n",
        "    num_labels=4,\n",
        "    id2label=id2label,\n",
        "    label2id=label2id,\n",
        "    ignore_mismatched_sizes=True,\n",
        ")\n",
        "\n",
        "# Augment بسيط لكنه فعال بدون ما يخرّب الواقعية\n",
        "def random_hflip(image, mask, p=0.5):\n",
        "    if np.random.rand() < p:\n",
        "        image = image.transpose(Image.FLIP_LEFT_RIGHT)\n",
        "        mask  = mask.transpose(Image.FLIP_LEFT_RIGHT)\n",
        "    return image, mask\n",
        "\n",
        "def random_color_jitter(image, p=0.5):\n",
        "    if np.random.rand() < p:\n",
        "        import torchvision.transforms as T\n",
        "        jitter = T.ColorJitter(brightness=0.15, contrast=0.15, saturation=0.10, hue=0.02)\n",
        "        image = jitter(image)\n",
        "    return image\n",
        "\n",
        "class RoomStructDataset(Dataset):\n",
        "    def __init__(self, pairs, train=True):\n",
        "        self.pairs = pairs\n",
        "        self.train = train\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.pairs)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path, msk_path = self.pairs[idx]\n",
        "        image = Image.open(img_path).convert(\"RGB\")\n",
        "        mask  = Image.open(msk_path)  # 0..3\n",
        "\n",
        "        if self.train:\n",
        "            image, mask = random_hflip(image, mask, p=0.5)\n",
        "            image = random_color_jitter(image, p=0.5)\n",
        "\n",
        "        enc = processor(image, mask, return_tensors=\"pt\")\n",
        "        return {k:v.squeeze(0) for k,v in enc.items()}\n",
        "\n",
        "train_ds = RoomStructDataset(train_pairs, train=True)\n",
        "val_ds   = RoomStructDataset(val_pairs, train=False)\n",
        "\n",
        "metric = evaluate.load(\"mean_iou\")\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    logits = torch.from_numpy(logits)\n",
        "    up = torch.nn.functional.interpolate(\n",
        "        logits, size=labels.shape[-2:], mode=\"bilinear\", align_corners=False\n",
        "    )\n",
        "    pred = up.argmax(dim=1).cpu().numpy()\n",
        "    out = metric.compute(\n",
        "        predictions=pred,\n",
        "        references=labels,\n",
        "        num_labels=4,\n",
        "        ignore_index=-1,\n",
        "        reduce_labels=False,\n",
        "    )\n",
        "    return {\"mean_iou\": out[\"mean_iou\"], \"mean_accuracy\": out[\"mean_accuracy\"]}\n",
        "\n",
        "SAVE_DIR = \"/content/drive/MyDrive/segformer_room_struct\"\n",
        "\n",
        "args = TrainingArguments(\n",
        "    output_dir=SAVE_DIR,\n",
        "    learning_rate=5e-5,\n",
        "    num_train_epochs=25,\n",
        "    per_device_train_batch_size=2,\n",
        "    per_device_eval_batch_size=2,\n",
        "    gradient_accumulation_steps=2,\n",
        "\n",
        "    eval_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "\n",
        "    lr_scheduler_type=\"cosine\",\n",
        "    warmup_ratio=0.06,\n",
        "    weight_decay=0.01,\n",
        "    fp16=torch.cuda.is_available(),\n",
        "    remove_unused_columns=False,\n",
        "    report_to=\"none\",\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"mean_iou\",\n",
        "    greater_is_better=True,\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=args,\n",
        "    train_dataset=train_ds,\n",
        "    eval_dataset=val_ds,\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "\n",
        "trainer.train()\n",
        "print(trainer.evaluate())\n",
        "\n",
        "trainer.save_model(os.path.join(SAVE_DIR, \"final\"))\n",
        "processor.save_pretrained(os.path.join(SAVE_DIR, \"final\"))\n",
        "print(\"Saved final model to:\", os.path.join(SAVE_DIR, \"final\"))\n"
      ],
      "metadata": {
        "id": "wwe0-hoeSXIJ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "375080e7-9f1b-4361-b383-7692ba49fa2c"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of SegformerForSemanticSegmentation were not initialized from the model checkpoint at nvidia/segformer-b2-finetuned-ade-512-512 and are newly initialized because the shapes did not match:\n",
            "- decode_head.classifier.weight: found shape torch.Size([150, 768, 1, 1]) in the checkpoint and torch.Size([4, 768, 1, 1]) in the model instantiated\n",
            "- decode_head.classifier.bias: found shape torch.Size([150]) in the checkpoint and torch.Size([4]) in the model instantiated\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='5106' max='7700' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [5106/7700 43:16 < 21:59, 1.97 it/s, Epoch 16.57/25]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Mean Iou</th>\n",
              "      <th>Mean Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.383223</td>\n",
              "      <td>0.717186</td>\n",
              "      <td>0.782658</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.694500</td>\n",
              "      <td>0.245850</td>\n",
              "      <td>0.784747</td>\n",
              "      <td>0.888320</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.694500</td>\n",
              "      <td>0.207123</td>\n",
              "      <td>0.810957</td>\n",
              "      <td>0.905783</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.245800</td>\n",
              "      <td>0.209224</td>\n",
              "      <td>0.809003</td>\n",
              "      <td>0.907206</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>0.180800</td>\n",
              "      <td>0.199084</td>\n",
              "      <td>0.812526</td>\n",
              "      <td>0.909700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>0.180800</td>\n",
              "      <td>0.221859</td>\n",
              "      <td>0.811621</td>\n",
              "      <td>0.901935</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>0.153600</td>\n",
              "      <td>0.209925</td>\n",
              "      <td>0.818627</td>\n",
              "      <td>0.912482</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>0.153600</td>\n",
              "      <td>0.217116</td>\n",
              "      <td>0.819809</td>\n",
              "      <td>0.907179</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>0.135000</td>\n",
              "      <td>0.228956</td>\n",
              "      <td>0.818382</td>\n",
              "      <td>0.910023</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>0.120100</td>\n",
              "      <td>0.226101</td>\n",
              "      <td>0.812077</td>\n",
              "      <td>0.909781</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11</td>\n",
              "      <td>0.120100</td>\n",
              "      <td>0.224815</td>\n",
              "      <td>0.817948</td>\n",
              "      <td>0.910913</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12</td>\n",
              "      <td>0.108200</td>\n",
              "      <td>0.244789</td>\n",
              "      <td>0.813501</td>\n",
              "      <td>0.919835</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13</td>\n",
              "      <td>0.100600</td>\n",
              "      <td>0.254181</td>\n",
              "      <td>0.817794</td>\n",
              "      <td>0.911052</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14</td>\n",
              "      <td>0.100600</td>\n",
              "      <td>0.239068</td>\n",
              "      <td>0.821969</td>\n",
              "      <td>0.910604</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>15</td>\n",
              "      <td>0.094100</td>\n",
              "      <td>0.241048</td>\n",
              "      <td>0.826011</td>\n",
              "      <td>0.910440</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>16</td>\n",
              "      <td>0.094100</td>\n",
              "      <td>0.256035</td>\n",
              "      <td>0.821808</td>\n",
              "      <td>0.910310</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/datasets/features/image.py:371: UserWarning: Downcasting array dtype int64 to int32 to be compatible with 'Pillow'\n",
            "  warnings.warn(f\"Downcasting array dtype {dtype} to {dest_dtype} to be compatible with 'Pillow'\")\n",
            "/usr/local/lib/python3.12/dist-packages/datasets/features/image.py:371: UserWarning: Downcasting array dtype int64 to int32 to be compatible with 'Pillow'\n",
            "  warnings.warn(f\"Downcasting array dtype {dtype} to {dest_dtype} to be compatible with 'Pillow'\")\n",
            "/usr/local/lib/python3.12/dist-packages/datasets/features/image.py:371: UserWarning: Downcasting array dtype int64 to int32 to be compatible with 'Pillow'\n",
            "  warnings.warn(f\"Downcasting array dtype {dtype} to {dest_dtype} to be compatible with 'Pillow'\")\n",
            "/usr/local/lib/python3.12/dist-packages/datasets/features/image.py:371: UserWarning: Downcasting array dtype int64 to int32 to be compatible with 'Pillow'\n",
            "  warnings.warn(f\"Downcasting array dtype {dtype} to {dest_dtype} to be compatible with 'Pillow'\")\n",
            "/usr/local/lib/python3.12/dist-packages/datasets/features/image.py:371: UserWarning: Downcasting array dtype int64 to int32 to be compatible with 'Pillow'\n",
            "  warnings.warn(f\"Downcasting array dtype {dtype} to {dest_dtype} to be compatible with 'Pillow'\")\n",
            "/usr/local/lib/python3.12/dist-packages/datasets/features/image.py:371: UserWarning: Downcasting array dtype int64 to int32 to be compatible with 'Pillow'\n",
            "  warnings.warn(f\"Downcasting array dtype {dtype} to {dest_dtype} to be compatible with 'Pillow'\")\n",
            "/usr/local/lib/python3.12/dist-packages/datasets/features/image.py:371: UserWarning: Downcasting array dtype int64 to int32 to be compatible with 'Pillow'\n",
            "  warnings.warn(f\"Downcasting array dtype {dtype} to {dest_dtype} to be compatible with 'Pillow'\")\n",
            "/usr/local/lib/python3.12/dist-packages/datasets/features/image.py:371: UserWarning: Downcasting array dtype int64 to int32 to be compatible with 'Pillow'\n",
            "  warnings.warn(f\"Downcasting array dtype {dtype} to {dest_dtype} to be compatible with 'Pillow'\")\n",
            "/usr/local/lib/python3.12/dist-packages/datasets/features/image.py:371: UserWarning: Downcasting array dtype int64 to int32 to be compatible with 'Pillow'\n",
            "  warnings.warn(f\"Downcasting array dtype {dtype} to {dest_dtype} to be compatible with 'Pillow'\")\n",
            "/usr/local/lib/python3.12/dist-packages/datasets/features/image.py:371: UserWarning: Downcasting array dtype int64 to int32 to be compatible with 'Pillow'\n",
            "  warnings.warn(f\"Downcasting array dtype {dtype} to {dest_dtype} to be compatible with 'Pillow'\")\n",
            "/usr/local/lib/python3.12/dist-packages/datasets/features/image.py:371: UserWarning: Downcasting array dtype int64 to int32 to be compatible with 'Pillow'\n",
            "  warnings.warn(f\"Downcasting array dtype {dtype} to {dest_dtype} to be compatible with 'Pillow'\")\n",
            "/usr/local/lib/python3.12/dist-packages/datasets/features/image.py:371: UserWarning: Downcasting array dtype int64 to int32 to be compatible with 'Pillow'\n",
            "  warnings.warn(f\"Downcasting array dtype {dtype} to {dest_dtype} to be compatible with 'Pillow'\")\n",
            "/usr/local/lib/python3.12/dist-packages/datasets/features/image.py:371: UserWarning: Downcasting array dtype int64 to int32 to be compatible with 'Pillow'\n",
            "  warnings.warn(f\"Downcasting array dtype {dtype} to {dest_dtype} to be compatible with 'Pillow'\")\n",
            "/usr/local/lib/python3.12/dist-packages/datasets/features/image.py:371: UserWarning: Downcasting array dtype int64 to int32 to be compatible with 'Pillow'\n",
            "  warnings.warn(f\"Downcasting array dtype {dtype} to {dest_dtype} to be compatible with 'Pillow'\")\n",
            "/usr/local/lib/python3.12/dist-packages/datasets/features/image.py:371: UserWarning: Downcasting array dtype int64 to int32 to be compatible with 'Pillow'\n",
            "  warnings.warn(f\"Downcasting array dtype {dtype} to {dest_dtype} to be compatible with 'Pillow'\")\n",
            "/usr/local/lib/python3.12/dist-packages/datasets/features/image.py:371: UserWarning: Downcasting array dtype int64 to int32 to be compatible with 'Pillow'\n",
            "  warnings.warn(f\"Downcasting array dtype {dtype} to {dest_dtype} to be compatible with 'Pillow'\")\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='7700' max='7700' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [7700/7700 1:03:14, Epoch 25/25]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Mean Iou</th>\n",
              "      <th>Mean Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.383223</td>\n",
              "      <td>0.717186</td>\n",
              "      <td>0.782658</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.694500</td>\n",
              "      <td>0.245850</td>\n",
              "      <td>0.784747</td>\n",
              "      <td>0.888320</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.694500</td>\n",
              "      <td>0.207123</td>\n",
              "      <td>0.810957</td>\n",
              "      <td>0.905783</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.245800</td>\n",
              "      <td>0.209224</td>\n",
              "      <td>0.809003</td>\n",
              "      <td>0.907206</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>0.180800</td>\n",
              "      <td>0.199084</td>\n",
              "      <td>0.812526</td>\n",
              "      <td>0.909700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>0.180800</td>\n",
              "      <td>0.221859</td>\n",
              "      <td>0.811621</td>\n",
              "      <td>0.901935</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>0.153600</td>\n",
              "      <td>0.209925</td>\n",
              "      <td>0.818627</td>\n",
              "      <td>0.912482</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>0.153600</td>\n",
              "      <td>0.217116</td>\n",
              "      <td>0.819809</td>\n",
              "      <td>0.907179</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>0.135000</td>\n",
              "      <td>0.228956</td>\n",
              "      <td>0.818382</td>\n",
              "      <td>0.910023</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>0.120100</td>\n",
              "      <td>0.226101</td>\n",
              "      <td>0.812077</td>\n",
              "      <td>0.909781</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11</td>\n",
              "      <td>0.120100</td>\n",
              "      <td>0.224815</td>\n",
              "      <td>0.817948</td>\n",
              "      <td>0.910913</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12</td>\n",
              "      <td>0.108200</td>\n",
              "      <td>0.244789</td>\n",
              "      <td>0.813501</td>\n",
              "      <td>0.919835</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13</td>\n",
              "      <td>0.100600</td>\n",
              "      <td>0.254181</td>\n",
              "      <td>0.817794</td>\n",
              "      <td>0.911052</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14</td>\n",
              "      <td>0.100600</td>\n",
              "      <td>0.239068</td>\n",
              "      <td>0.821969</td>\n",
              "      <td>0.910604</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>15</td>\n",
              "      <td>0.094100</td>\n",
              "      <td>0.241048</td>\n",
              "      <td>0.826011</td>\n",
              "      <td>0.910440</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>16</td>\n",
              "      <td>0.094100</td>\n",
              "      <td>0.256035</td>\n",
              "      <td>0.821808</td>\n",
              "      <td>0.910310</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>17</td>\n",
              "      <td>0.091600</td>\n",
              "      <td>0.251638</td>\n",
              "      <td>0.823666</td>\n",
              "      <td>0.914771</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>18</td>\n",
              "      <td>0.086800</td>\n",
              "      <td>0.247884</td>\n",
              "      <td>0.826208</td>\n",
              "      <td>0.909994</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>19</td>\n",
              "      <td>0.086800</td>\n",
              "      <td>0.250562</td>\n",
              "      <td>0.826851</td>\n",
              "      <td>0.908220</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>0.083800</td>\n",
              "      <td>0.249476</td>\n",
              "      <td>0.825199</td>\n",
              "      <td>0.915218</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>21</td>\n",
              "      <td>0.083800</td>\n",
              "      <td>0.253995</td>\n",
              "      <td>0.824106</td>\n",
              "      <td>0.913086</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>22</td>\n",
              "      <td>0.081400</td>\n",
              "      <td>0.257913</td>\n",
              "      <td>0.822847</td>\n",
              "      <td>0.914759</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>23</td>\n",
              "      <td>0.080500</td>\n",
              "      <td>0.253114</td>\n",
              "      <td>0.824117</td>\n",
              "      <td>0.912844</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>24</td>\n",
              "      <td>0.080500</td>\n",
              "      <td>0.266163</td>\n",
              "      <td>0.822389</td>\n",
              "      <td>0.913836</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>25</td>\n",
              "      <td>0.080400</td>\n",
              "      <td>0.262133</td>\n",
              "      <td>0.822913</td>\n",
              "      <td>0.914463</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/datasets/features/image.py:371: UserWarning: Downcasting array dtype int64 to int32 to be compatible with 'Pillow'\n",
            "  warnings.warn(f\"Downcasting array dtype {dtype} to {dest_dtype} to be compatible with 'Pillow'\")\n",
            "/usr/local/lib/python3.12/dist-packages/datasets/features/image.py:371: UserWarning: Downcasting array dtype int64 to int32 to be compatible with 'Pillow'\n",
            "  warnings.warn(f\"Downcasting array dtype {dtype} to {dest_dtype} to be compatible with 'Pillow'\")\n",
            "/usr/local/lib/python3.12/dist-packages/datasets/features/image.py:371: UserWarning: Downcasting array dtype int64 to int32 to be compatible with 'Pillow'\n",
            "  warnings.warn(f\"Downcasting array dtype {dtype} to {dest_dtype} to be compatible with 'Pillow'\")\n",
            "/usr/local/lib/python3.12/dist-packages/datasets/features/image.py:371: UserWarning: Downcasting array dtype int64 to int32 to be compatible with 'Pillow'\n",
            "  warnings.warn(f\"Downcasting array dtype {dtype} to {dest_dtype} to be compatible with 'Pillow'\")\n",
            "/usr/local/lib/python3.12/dist-packages/datasets/features/image.py:371: UserWarning: Downcasting array dtype int64 to int32 to be compatible with 'Pillow'\n",
            "  warnings.warn(f\"Downcasting array dtype {dtype} to {dest_dtype} to be compatible with 'Pillow'\")\n",
            "/usr/local/lib/python3.12/dist-packages/datasets/features/image.py:371: UserWarning: Downcasting array dtype int64 to int32 to be compatible with 'Pillow'\n",
            "  warnings.warn(f\"Downcasting array dtype {dtype} to {dest_dtype} to be compatible with 'Pillow'\")\n",
            "/usr/local/lib/python3.12/dist-packages/datasets/features/image.py:371: UserWarning: Downcasting array dtype int64 to int32 to be compatible with 'Pillow'\n",
            "  warnings.warn(f\"Downcasting array dtype {dtype} to {dest_dtype} to be compatible with 'Pillow'\")\n",
            "/usr/local/lib/python3.12/dist-packages/datasets/features/image.py:371: UserWarning: Downcasting array dtype int64 to int32 to be compatible with 'Pillow'\n",
            "  warnings.warn(f\"Downcasting array dtype {dtype} to {dest_dtype} to be compatible with 'Pillow'\")\n",
            "/usr/local/lib/python3.12/dist-packages/datasets/features/image.py:371: UserWarning: Downcasting array dtype int64 to int32 to be compatible with 'Pillow'\n",
            "  warnings.warn(f\"Downcasting array dtype {dtype} to {dest_dtype} to be compatible with 'Pillow'\")\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='109' max='109' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [109/109 00:10]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/datasets/features/image.py:371: UserWarning: Downcasting array dtype int64 to int32 to be compatible with 'Pillow'\n",
            "  warnings.warn(f\"Downcasting array dtype {dtype} to {dest_dtype} to be compatible with 'Pillow'\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'eval_loss': 0.25056225061416626, 'eval_mean_iou': 0.8268510781330647, 'eval_mean_accuracy': 0.9082203592141851, 'eval_runtime': 16.7283, 'eval_samples_per_second': 13.032, 'eval_steps_per_second': 6.516, 'epoch': 25.0}\n",
            "Saved final model to: /content/drive/MyDrive/segformer_room_struct/final\n"
          ]
        }
      ]
    }
  ]
}